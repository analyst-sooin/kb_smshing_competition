{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install customized_konlpy","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from konlpy.tag import Komoran,Okt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"komoran = Komoran()\nkomoran.morphs('점심은부대찌개입니다')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"import pandas as pd\ntrain = pd.read_csv(\"../input/train.csv\")\ntrain.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['smishing'].value_counts()\n# 0과1의 비율이 6.3% 이기 때문에 validation도 함께 비율을 맞춰줘야한다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test = pd.read_csv(\"../input/public_test.csv\")\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"*딥러닝 텍스트 처리 중요한 것.\n\n머신러닝, 딥러닝에서는 모든 데이터가  숫자로 바뀌어 있어야한다.(text든 이미지든,음성이든 예외 없이 모두다)\n\n한 문장의 하나하나의 단어 기준을 세분화 하고 각각 숫자로 만들어 줘서 등록해야한다(이 과정이 look up table. ex) '고객님' : 1, 'OO은행' : 2 , 연락처:3"},{"metadata":{"trusted":true},"cell_type":"code","source":"# komoran.morphs(train['text'])\n# # seires 형태로 되있어서 텍스트로 바꿔줘야함\n# # apply 함수를 써서 적용함 (for문 느림)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train['komoran_text']=train['text'].apply(lambda x : komoran.morphs(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['komoran_text']=test['text'].apply(lambda x : komoran.morphs(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.text import Tokenizer\n\ntk = Tokenizer()\ntk.fit_on_texts(list(train['komoran_text'])+list(test['komoran_text']))\n\ntk.word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_text = tk.texts_to_sequences(train['komoran_text'])\ntest_text = tk.texts_to_sequences(test['komoran_text'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(train_text).apply(len)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(tk.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 등록된 문자를 숫자로 바꿔줘야함(tk안에만등록)\ntrain_text = tk.texts_to_sequences(train['text']) # train의 text를 숫자로 바꿔서 변수에 저장\n\ntest_text = tk.texts_to_sequences(test['text']) # test의 text를 숫자로 바꿔서 변수에 저장\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 각각의 데이터에 칼럼의 갯수가 똑같아야지 모델이 학습을 시작할수있음.\n# 가장 길 길이의 텍스트에 맞추면 됨.\n# 여기선 가장큰 길이의 문자가 train의 317 이기때문에 317에 맞춰야함.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 1. 텍스트길이를 317 맥스값에 맞출것이아니라 잘라준다\n# ->데이터가 날아가면 손해일것같지만 다른 데이터들이 많아서 괜찮다.\n# -> 맥시멈값을 줄여주면 효율이좋아진다 -> 다른데이터들과의 길이 갭차이가 작아져서","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 대부분의 텍스트대회의 데이터는 뒷쪽 데이터가 훨씬중요하다(한국말은 끝까지 들어봐야한다.)\n# 뒷쪽의 데이터를 남겨두는게 중요하다.\n# 기본값이 앞쪽의 데이터를 날리도록 되어있다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['smishing'] == 1]['text']\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\na,b = plt.subplots(1,1,figsize=(20,10)) #밑그림\nsns.distplot(pd.Series(train_text).apply(len)) #분포표 displot\n# 분포를 보고 length 값을 몇으로 할것인지 가장 효율적인지 보고 진행해준다","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(train_text).apply(lambda x:len(x)).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pd.Series(test_text).apply(lambda x:len(x)).max()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train[train['smishing'] == 1]['text']\n\n# 영어와는 다르게 한국말 특성상 뒤에 단어에 중요한 정보다 많이 있기때문에 앞에 문장들을 잘라서 사용했다(문자길이 100이상인것들만)\n# 하지만 학습데이터의 라벨이 스미싱인('1') 데이터들만 모와서 봤더니, 맨 앞문장에 '(광고)'라는 키워드가 들어간걸 확인 할수있다.\n# 그러므로 '(광고)' 키워드가 스미싱을 학습하는데 아주 중요한 역할을 하는데 없애면 안된다.\n# 앞에 문장을 살리고 뒷쪽데이터만 날리는 학습파일(csv파일) 을 만들어서 두개 앙상블을 진행한다. 앞에 날린데이터와 뒤에날린데이터 모두 중요하기 떄문","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize =(20,12))\nsns.distplot(pd.Series(train_text).apply(len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize =(20,12))\nsns.distplot(pd.Series(test_text).apply(len))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#모든 데이터의 단어의 갯수를 맞춰줘야함\nfrom keras.preprocessing.sequence import pad_sequences\n\npad_train = pad_sequences(train_text, maxlen = 280)\npad_test = pad_sequences(test_text, maxlen =280)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras import Sequential\nfrom keras.layers import Dense,Embedding,Flatten ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_vaild,y_train,y_vaild = train_test_split(pad_train,train['smishing'],random_state=2020,test_size=0.15,stratify=train['smishing'])\n#random_state 값을 바꾸면서 val_loss가 제일 낮게 나오는것을 찾는다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel.add(Embedding(len(tk.word_index)+1,60,input_length=280))\n\n\nmodel.add(Flatten())\n\nmodel.add(Dense(2,activation ='softmax'))\n#sigmoid를 쓰면 0으로 생성된다.\n\nmodel.compile(loss = 'sparse_categorical_crossentropy',optimizer = 'adam',metrics= ['acc']) # 모델쌓기 끝났다고 선언해줘야함. \n# 분류모델을 해줄때는 y값을 분류로 one-hot-encoding을 해줘야하는데 안해줘서 회귀로 인식할수있어서 sparse 넣어준다","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint \n\nes = EarlyStopping(patience = 3) # 옵션이많다.알아보기, 이미지할때 loss가 크면 많이 참을수있도록 patience를 5이상으로 놓는다 텍스트는 안정적인편이여서 괜찮\nmc = ModelCheckpoint('best.h5',save_best_only = True) #최적의 순간만 저장하겠다.\n\nmodel.fit(x_train,y_train,epochs =20,validation_data=(x_vaild,y_vaild),batch_size=128,callbacks=[es,mc]) \n\n# 옵션값 1.은 학습횟수를 결정해줌 5번 반복횟수 2. 전체데이터셋의 10%만 학습에 이용하지않고 평가점수에만 이용함\n# 학습에 이용하지 않는 이유는 overfitting.\n# batch_size 는 속도조절(빠르게함)\n\n#new\n#학습에이용하지 않는 데이터를 정확한 비율로 (train_test_split에서 바꾼 값(stratify))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# pre train embedding을 가져와서 우리 데이터셋에 적용해서 하면 가장 모델의 최적이 됨(위키피디아라다던지,페이스북이라던지)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.load_weights('best.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(pad_test,batch_size=128)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['smishing'] = result\nsub = test[['id','smishing']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sub.to_csv('kb_competition_final.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\npad_train = pad_sequences(train_text,maxlen = 100,truncating='post',padding='post')\npad_train\n# 그래프를 보니 문자열 길이가 100 이하로만 설정해도 학습을 할 수 있다.(100이하인 문자열 데이터들이 매우많기때문에 효율적인 학습을 위해서 100보다 문자열이 긴 것들은 날려버린다.)\n# truncating은 뒷쪽을 자르고, padding은 뒷쪽을 0으로 채운다","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pad_test = pad_sequences(test_text,maxlen = 100,truncating='post',padding='post')\npad_test","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nx_train,x_vaild,y_train,y_vaild = train_test_split(pad_train,train['smishing'],test_size = 0.15,random_state=33,stratify=train['smishing'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 딥러닝 모델을 쌓는 방법 두가지꼭 필요함\nfrom keras import Sequential # 1. 가볍고 쉽고 간단하게 하나씩 쌓는다\nfrom keras.layers import Dense,Embedding,Flatten  \n# 가장 많이 쓰는 층 불러온다 , # 텍스트는 임베드 층이 반드시있어야함 어떤 텍스트든,# 차원을 축소시킴(flatten)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# 모델쌓기를 시작하겠습니다.\nmodel = Sequential() # 객체생성\n\nmodel.add(Embedding(448903,25,input_length=100)) \n# 입력층을 쌓겠다. 옵션 3가지 넣어야함(1. 전체데이터셋에대한 단어종류의 갯수(len(tk.word_index)),2.차원의 갯수를 설정,3.하나의 데이터당 몇개의 단어가 몇개가들어오는지 크기설정)\n\nmodel.add(Flatten())\n# 2차원인 애를 flatten 통해서 1차원으로 바꾸워줌(딥러닝에서는 네트워크 구조 연결이 중요하다)\n\nmodel.add(Dense(5,activation = 'relu'))\nmodel.add(Dense(1,activation ='sigmoid')) \n# 스미잉 인지 아닌지 출력이 되어야함(확률값으로)\n# 출력층,옵션 2가지 (1.정답클래스의 갯수가 들어가야함, )\n# 정답클래스의 갯수를 1을 넣었을경우엔(어떠한 확률) sigmoid 활성화 함수를, 2를 넣었을때는 softmax 활성화 함수를 넣어줘야함\n\nmodel.compile(loss = 'binary_crossentropy',optimizer = 'adam',metrics= ['acc']) # 모델쌓기 끝났다고 선언해줘야함. \n# 옵션 3가지 \n# 1.loss = 'sparse_categorycalcrossentropy'는 다중분류 썼을때 sparse_를쓰면 회귀가아닌 분류문제로 인식함(one-hot-encoding 할 필요없음)(필수)\n# 2.optimizer = adam은학습효과 빠르고 학습 잘함 (필수)\n# 3.metrics = ['acc]' <- 평가방식은 정확도로 보겠다.(선택)\n# 딥러닝은 w 만 잘찾으면 됨!(어떻게? -> 손실함수값이 작아지는 방향으로 찾겠다.)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.callbacks import EarlyStopping, ModelCheckpoint \n# 자동으로 모델이 validation loss가 안좋아지면 이정도면 과적합이 일어나구 있구나 하고 자동으로 멈췸\n# 모델이 최적의 순간을 저장해야되는데 학습시 어느순간 안좋아진다 그래서,최적의 가중치를 가지도록 \n# 가장 점수가 잘 나올 가중치를 저장하고 예측에 사용한다.-> modelcheckpoint\n\nes = EarlyStopping(patience = 3) #이미지할때 loss가 크면 많이 참을수있도록 patience를 5이상으로 놓는다 텍스트는 안정적인편이여서 괜찮\nmc = ModelCheckpoint('best.h5',save_best_only = True) #최적의 순간만 저장하겠다.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(pad_train,train['smishing'],epochs =20,validation_data = (x_vaild,y_vaild),batch_size=128,callbacks=[es,mc]) \n\n# 옵션값 1.은 학습횟수를 결정해줌 5번 반복횟수 2. 전체데이터셋의 10%만 학습에 이용하지않고 평가점수에만 이용함\n# 학습에 이용하지 않는 이유는 overfitting.\n# batch_size 는 속도조절(빠르게함)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result = model.predict(pad_test,batch_size=128)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['smishing'] = result\nsub = test[['id','smishing']] #컬럼여러개로 접근할때 리스트괄호 2개\nsub.to_csv('kb_competition3.csv',index = False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}